# -*- coding: utf-8 -*-
"""0424.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rga48qQsAiHdKmdUBaOxn3vbblWQfEht
"""

import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from shapely.geometry import box
from shapely.geometry import Point
from shapely.ops import unary_union
from matplotlib.patches import Patch
import numpy as np

########################################
# 0) Shapefile ë¡œë“œ & 'Sector'ë¥¼ intë¡œ í†µì¼
########################################
shapefile_path = "Balanced_Beats_V2.shp"
berkeley_beats = gpd.read_file(shapefile_path).to_crs(epsg=4326)

# ğŸ”¥ ë§Œì•½ shapefileì˜ 'Sector'ê°€ ë¬¸ìì—´ì´ë©´, ì•„ë˜ ì½”ë“œë¡œ int ë³€í™˜
berkeley_beats["Sector"] = berkeley_beats["Sector"].astype(int)

########################################
# 1) Grid ìƒì„± & sjoinìœ¼ë¡œ Sector í• ë‹¹
########################################
minx, miny, maxx, maxy = berkeley_beats.total_bounds
cell_size = 0.001  # 100m ê²©ì

grid_cells = []
x = minx
while x < maxx:
    y = miny
    while y < maxy:
        cell = box(x, y, x + cell_size, y + cell_size)
        grid_cells.append(cell)
        y += cell_size
    x += cell_size

grid = gpd.GeoDataFrame(geometry=grid_cells, crs=berkeley_beats.crs)

# Centroid ê³„ì‚° & Spatial Join
grid["centroid"] = grid.geometry.centroid
centroids = grid.copy()
centroids["geometry"] = centroids["centroid"]

centroids_sjoined = gpd.sjoin(
    centroids,
    berkeley_beats,
    how="left",
    predicate="intersects"
).reset_index(drop=True)

grid["Sector"] = centroids_sjoined["Sector"]

# ê²°ì¸¡ ì œê±° & Grid_ID ë¶€ì—¬
grid = grid.dropna(subset=["Sector"])
grid["Sector"] = grid["Sector"].astype(int)  # ğŸ”¥ ì •ìˆ˜ ë³€í™˜ (í†µì¼)
grid["Grid_ID"] = grid.index

########################################
# 2) ê¸°ë³¸ ì‹œê°í™” (Sectorë³„ Grid)
########################################
fig, ax = plt.subplots(figsize=(12,8))
unique_sectors = sorted(grid["Sector"].unique())
cmap = plt.cm.get_cmap("tab20", len(unique_sectors))
color_dict = {sec: cmap(i) for i, sec in enumerate(unique_sectors)}

for sec in unique_sectors:
    subset = grid[grid["Sector"] == sec]
    subset.plot(ax=ax, facecolor=color_dict[sec], edgecolor="black", linewidth=0.5, alpha=1.0)

legend_patches = [
    Patch(facecolor=color_dict[sec], edgecolor="black", label=f"Sector {sec}")
    for sec in unique_sectors
]
ax.legend(
    handles=legend_patches,
    title="Centroid-Based Sector Assignment",
    loc="upper left",
    bbox_to_anchor=(1.05, 1),
    borderaxespad=0.
)

ax.set_title("Grid-Based Patrol Sector Assignment")
ax.set_axis_off()
plt.show()

########################################
# 3) Boundary Grid ì‹ë³„
########################################
boundary_grids = grid.copy()
boundary_grids["is_boundary"] = False

for idx, row in grid.iterrows():
    neighbors = grid[grid.geometry.touches(row.geometry)]
    if not neighbors.empty:
        if any(neighbors["Sector"] != row["Sector"]):
            boundary_grids.at[idx, "is_boundary"] = True

# boundary_grids_info: ê²½ê³„ì¸ ê²ƒë§Œ
boundary_grids_info = boundary_grids[boundary_grids["is_boundary"]].copy()

# ğŸ”¥ intí˜• Sector ìœ ì§€
boundary_grids_info["Sector"] = boundary_grids_info["Sector"].astype(int)

# load year data
import pandas as pd
df = pd.read_csv("2024.csv")

# âœ… Count duplicate (lat, lon) rows
duplicate_coords = df.duplicated(subset=['lat', 'lon'], keep=False).sum()

# âœ… Total number of rows
total_rows = len(df)

# âœ… Calculate duplicate percentage
duplicate_ratio = (duplicate_coords / total_rows) * 100

# âœ… Print results
print(f"Number of rows with duplicate (lat, lon) coordinates: {duplicate_coords}")
print(f"Percentage of duplicate rows in the dataset: {duplicate_ratio:.2f}%")

unique_coords = df[['lat', 'lon']].drop_duplicates()
print(f"Unique (lat, lon) pairs: {len(unique_coords)}")

# âœ… Count occurrences of each (lat, lon) pair
coord_counts = df.groupby(['lat', 'lon']).size().reset_index(name='count')

# âœ… Sort by most frequent duplicates
coord_counts = coord_counts.sort_values(by='count', ascending=False)

# âœ… Select top 10
top_10_coords = coord_counts.head(10)

# âœ… Display the result
print(top_10_coords)

df = df[~((df['lat'] == 37.87053067971685) & (df['lon'] == -122.273288))]
df.shape

import pandas as pd
import numpy as np
import re
from sklearn.preprocessing import MinMaxScaler

# 2ï¸âƒ£ **í•„ìš”í•œ ì»¬ëŸ¼ ì„ íƒ ë° ë³µì‚¬ë³¸ ìƒì„±**
selected_columns = ['Priority', 'lat', 'lon', 'Time Spent Responding', 'Dispositions']
new_df = df[selected_columns].copy()  # ğŸš¨ ë³µì‚¬ë³¸ ìƒì„±

# 4ï¸âƒ£ **IQR ì´ìƒì¹˜ ê°’ì„ 0ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” í•¨ìˆ˜**
def replace_outliers_with_zero(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # ì´ìƒì¹˜ì¸ ê²½ìš° 0ìœ¼ë¡œ ì„¤ì •
    df[column] = df[column].apply(lambda x: 0 if x < lower_bound or x > upper_bound else x)
    return df

# âœ… **ì´ìƒì¹˜ ê°’ì„ 0ìœ¼ë¡œ ë³€ê²½ ì ìš©**
new_df = replace_outliers_with_zero(new_df, 'Time Spent Responding')

# 5ï¸âƒ£ **Priority ìˆ«ì ë³€í™˜ í•¨ìˆ˜**
def extract_priority(level):
    if pd.isna(level) or level.strip() == "":
        return 0
    match = re.search(r'(\d+)', level)
    if match:
        priority = match.group(1)
        return 1 if priority == "1F" else int(priority)
    return 0

# âœ… **Priority ìˆ«ì ë³€í™˜ í›„ ì»¬ëŸ¼ ì¶”ê°€**
new_df['Priority Numeric'] = new_df['Priority'].apply(extract_priority)

# 6ï¸âƒ£ **Priority Weight ë§¤í•‘**
priority_weights = {1: 1.0, 2: 0.7, 3: 0.4, 4: 0.2, 5: 0.1}
new_df['Priority Weight'] = new_df['Priority Numeric'].apply(lambda x: priority_weights.get(x, 0.0))

# 7ï¸âƒ£ **Min-Max Scaling ì ìš©**
scaler = MinMaxScaler()
new_df['Scaled Response Time'] = scaler.fit_transform(new_df[['Time Spent Responding']])

# 8ï¸âƒ£ **Dispositions ê°€ì¤‘ì¹˜ ì ìš©**
def get_disposition_weight(disposition):
    if pd.isna(disposition) or disposition.strip() == "":
        return 0.3
    disposition = disposition.lower()
    if "arrest" in disposition:
        return 1.0
    elif "case" in disposition:
        return 0.7
    else:
        return 0.3

# âœ… **Disposition Weight ì»¬ëŸ¼ ì¶”ê°€**
new_df['Disposition Weight'] = new_df['Dispositions'].apply(get_disposition_weight)

# 1ï¸âƒ£ NPPS ì ìˆ˜ ê³„ì‚° (ê° ê°€ì¤‘ì¹˜ëŠ” ì¡°ì • ê°€ëŠ¥)
new_df["NPPS"] = (0.7 * new_df["Priority Weight"]) + (0.1 * new_df["Scaled Response Time"]) + (0.2 * new_df["Disposition Weight"])

#
total_npps = new_df['NPPS'].sum()
print("Total_NPPS(SUM)", total_npps)

# NPPS ê°’ì˜ í†µê³„ ìš”ì•½
npps_min = new_df["NPPS"].min()
npps_max = new_df["NPPS"].max()
npps_mean = new_df["NPPS"].mean()

# ê²°ê³¼ ì¶œë ¥
print(f"Minimum NPPS: {npps_min:.4f}")
print(f"Maximum NPPS: {npps_max:.4f}")
print(f"Average NPPS: {npps_mean:.4f}")


# ê²°ê³¼ í™•ì¸
new_df.head()

npps_data = new_df.copy()

# 1ï¸âƒ£ npps_data ë¶ˆëŸ¬ì˜¤ê¸° (ì„œë¹„ìŠ¤ ì½œ ì¢Œí‘œ ë° NPPS ê°’ í¬í•¨)
npps_data = gpd.GeoDataFrame(npps_data, geometry=gpd.points_from_xy(npps_data['lon'], npps_data['lat']), crs="EPSG:4326")

# 2ï¸âƒ£ "nearest" ë°©ì‹ìœ¼ë¡œ ì„œë¹„ìŠ¤ ì½œì„ ê°€ì¥ ê°€ê¹Œìš´ Gridì— í• ë‹¹
npps_data = gpd.sjoin_nearest(npps_data, grid, how="left", distance_col="distance_to_grid")

# 3ï¸âƒ£ Gridë³„ NPPS í•©ê³„ ê³„ì‚°
grid_npps_sum = npps_data.groupby("Grid_ID")["NPPS"].sum().reset_index()
grid_npps_sum.rename(columns={"NPPS": "total_npps"}, inplace=True)  # ì»¬ëŸ¼ëª… ë³€ê²½

# 4ï¸âƒ£ Grid ë°ì´í„°ì™€ NPPS ê°’ì„ ë³‘í•©í•˜ì—¬ ì‹œê°í™” ì¤€ë¹„
grid = grid.merge(grid_npps_sum, on="Grid_ID", how="left")

# 5ï¸âƒ£ NPPS ê°’ì´ NaNì¸ ê²½ìš° 0ìœ¼ë¡œ ì±„ìš°ê¸° (ì„œë¹„ìŠ¤ ì½œì´ ì—†ëŠ” Grid ë•Œë¬¸)
grid["total_npps"] = grid["total_npps"].fillna(0)

########################################
# 5) NPPS Heatmap ì‹œê°í™” (Grid ë‹¨ìœ„)
########################################

# 6ï¸âƒ£ Heatmap ì‹œê°í™”
fig, ax = plt.subplots(figsize=(12,8))

# Normalize NPPS ê°’ (ìƒ‰ìƒ ê°•ë„ ì¡°ì ˆ)
cmap = plt.cm.get_cmap("Reds")  # ë¹¨ê°„ìƒ‰ ê³„ì—´ë¡œ heatmap
norm = plt.Normalize(grid["total_npps"].min(), grid["total_npps"].max())

# Grid ê¸°ë°˜ Heatmap ê·¸ë¦¬ê¸°
grid.plot(column="total_npps", cmap=cmap, norm=norm, linewidth=0.5, edgecolor="black", alpha=0.8, ax=ax)

# ì»¬ëŸ¬ë°” ì¶”ê°€
sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
sm.set_array([])
cbar = plt.colorbar(sm, ax=ax, fraction=0.03, pad=0.02)
cbar.set_label("Total WLS per Grid", fontsize=12)

# ì œëª© ë° ì„¤ì •
ax.set_title("Aggregated WLS Scores by Grid Cell", fontsize=14)
ax.set_axis_off()  # ì¶• ì œê±°
plt.show()

########################################
# 6) Sectorë³„ NPPS í•©ê³„ & ì‹œê°í™” (Polygon ë‹¨ìœ„)
########################################
sector_npps_sum = grid.groupby("Sector")["total_npps"].sum().reset_index()
sector_npps_sum.rename(columns={"total_npps": "sector_total_npps"}, inplace=True)

# berkeley_beatsì™€ merge (ë‘˜ ë‹¤ intí˜• Sector)
berkeley_beats = berkeley_beats.merge(sector_npps_sum, on="Sector", how="left")

print("\nğŸ“Š **Sectorë³„ NPPS ì´í•©** ğŸ“Š\n")
print(berkeley_beats[["Sector", "sector_total_npps"]])

fig, ax = plt.subplots(figsize=(12, 8))
cmap = plt.cm.Reds
norm = mcolors.Normalize(vmin=berkeley_beats["sector_total_npps"].min(),
                         vmax=berkeley_beats["sector_total_npps"].max())

berkeley_beats.plot(column="sector_total_npps", cmap=cmap, norm=norm,
                    edgecolor="black", linewidth=1, alpha=0.8, ax=ax)

# Sector ì¤‘ì‹¬ì  ë¼ë²¨ í‘œì‹œ
berkeley_beats["centroid"] = berkeley_beats.geometry.centroid
for idx, row in berkeley_beats.iterrows():
    ax.text(row["centroid"].x, row["centroid"].y,
            f"{int(row['Sector'])}",
            fontsize=10, color="black", ha="center", va="center", fontweight="bold")

sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
sm.set_array([])
cbar = plt.colorbar(sm, ax=ax, fraction=0.03, pad=0.02)
cbar.set_label("Total WLS per Sector", fontsize=12)

ax.set_title("Sector-Level WLS Heatmap in Berkeley Police Patrol", fontsize=14)
ax.set_axis_off()
plt.show()

# 1ï¸âƒ£ NPPS ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ Top 10 Grid ì°¾ê¸°
top_10_hotspots = grid.nlargest(10, "total_npps")[["Grid_ID", "total_npps", "geometry", "Sector"]]

# 2ï¸âƒ£ Top 10 í•«ìŠ¤íŒŸì˜ ì¤‘ì‹¬ ì¢Œí‘œ ê³„ì‚° (ê° Gridì˜ ì¤‘ì‹¬ì )
top_10_hotspots["centroid"] = top_10_hotspots.geometry.centroid

# 3ï¸âƒ£ ğŸ“Œ Top 10 í•«ìŠ¤íŒŸ Grid ì •ë³´ ì¶œë ¥
print(" **Top 10 NPPS Hotspot Grids** ")
print(top_10_hotspots[["Grid_ID", "total_npps", "Sector"]])

# 5ï¸âƒ£ ğŸ”¥ Sector-Level NPPS Heatmap + ğŸ† Top 10 Hotspot Overlay
fig, ax = plt.subplots(figsize=(12, 8))

# ğŸ’¡ íˆíŠ¸ë§µ ìƒ‰ìƒ ì¡°ì • (ë¶‰ì€ìƒ‰ ë†ë„ ê°•ì¡°)
cmap = plt.cm.Reds
norm = mcolors.Normalize(vmin=berkeley_beats["sector_total_npps"].min(), vmax=berkeley_beats["sector_total_npps"].max())

# ğŸ—ºï¸ **Sectorë³„ NPPS ì‹œê°í™”**
berkeley_beats.plot(column="sector_total_npps", cmap=cmap, norm=norm, edgecolor="black", linewidth=1, alpha=0.8, ax=ax)

# Sector ì¤‘ì‹¬ì  ë¼ë²¨ í‘œì‹œ
berkeley_beats["centroid"] = berkeley_beats.geometry.centroid
for idx, row in berkeley_beats.iterrows():
    ax.text(row["centroid"].x, row["centroid"].y,
            f"{int(row['Sector'])}",
            fontsize=10, color="black", ha="center", va="center", fontweight="bold")

# ğŸ”¥ **Top 10 Hotspot Grid ì¤‘ì‹¬ ì¢Œí‘œ ì¶”ê°€ (ë¹¨ê°„ ì›)**
top_10_hotspots["centroid"].plot(ax=ax, color="red", markersize=100, marker="o", label="Top 10 WLS Hotspots")

# ì»¬ëŸ¬ë°” ì¶”ê°€
sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
sm.set_array([])
cbar = plt.colorbar(sm, ax=ax, fraction=0.03, pad=0.02)
cbar.set_label("Total WLS per Sector", fontsize=12)

# ë²”ë¡€ ì¶”ê°€
ax.legend()
ax.set_title(" Sector-Level WLS Heatmap with Top 10 Hotspots", fontsize=14)
ax.set_axis_off()  # ì¶• ì œê±°
plt.show()

########################################
# 7) Excess/Deficient íŒë‹¨
########################################
sector_npps_sum["Sector"] = sector_npps_sum["Sector"].astype(int)  # ì´ë¯¸ intì§€ë§Œ í˜¹ì‹œ ëª¨ë¦„
sector_npps_sum = sector_npps_sum.sort_values("Sector").reset_index(drop=True)

print("\nğŸ“Š **Total NPPS per Sector (Sorted)** ğŸ“Š\n")
print(sector_npps_sum)

sector_npps_mean = sector_npps_sum["sector_total_npps"].mean()
print(f"\nğŸ“Š **Average NPPS Across 14 Sectors: {sector_npps_mean:.2f}** ğŸ“Š\n")

threshold_high = sector_npps_mean * 1.2
threshold_low  = sector_npps_mean * 0.8

sector_npps_sum["npps_difference(%)"] = ((sector_npps_sum["sector_total_npps"] - sector_npps_mean)
                                         / sector_npps_mean) * 100

excess_npps = sector_npps_sum[sector_npps_sum["sector_total_npps"] > threshold_high]
deficient_npps = sector_npps_sum[sector_npps_sum["sector_total_npps"] < threshold_low]

print("\nğŸ”¥ **Excess NPPS Sectors (Above +20%)** ğŸ”¥")
print(excess_npps.to_string(index=False))

print("\nğŸ”µ **Deficient NPPS Sectors (Below -20%)** ğŸ”µ")
print(deficient_npps.to_string(index=False))

########################################
# 8) boundary_grids_info & sector_npps_sum ë³‘í•©
########################################
# boundary_grids_info: ê²½ê³„ Gridë§Œ ëª¨ì•„ë‘” DF
# 'Sector'ë„ ì´ë¯¸ int ìƒíƒœ

boundary_grids_info["Sector"] = boundary_grids_info["Sector"].astype(int)
boundary_grids_info = boundary_grids_info.merge(
    sector_npps_sum[["Sector", "sector_total_npps"]],
    on="Sector",
    how="left"
)

boundary_grids_info.rename(columns={
    "total_npps": "grid_npps",
    "sector_total_npps": "sector_npps"
}, inplace=True)

print("\nğŸ” **Boundary Grids Info** ğŸ”\n")
print(boundary_grids_info.head(10))

########################################
# 9) ê²½ê³„ Grid ì‹œê°í™”
########################################
fig, ax = plt.subplots(figsize=(12,8))
grid.plot(ax=ax, facecolor="lightgrey", edgecolor="black", linewidth=0.2, alpha=0.6)

boundary_grids_info.plot(ax=ax, facecolor="red", edgecolor="black",
                         linewidth=0.5, alpha=1.0, label="Boundary Grids")

berkeley_beats.boundary.plot(ax=ax, color="black", linewidth=1.5, linestyle="--",
                             label="Sector Boundaries")

ax.legend()
ax.set_title("Patrol Sector Grid with Boundary Highlights", fontsize=14)
ax.set_axis_off()
plt.show()

########################################
# 10) Sectorë³„ë¡œ ë‹¤ë¥¸ ìƒ‰ê¹”ë¡œ ê²½ê³„ Grid ì‹œê°í™”
########################################
fig, ax = plt.subplots(figsize=(12, 8))
grid.plot(ax=ax, facecolor="lightgrey", edgecolor="black", linewidth=0.2, alpha=0.6)

unique_sectors_bdry = boundary_grids_info["Sector"].unique()
cmap2 = plt.cm.get_cmap("tab20", len(unique_sectors_bdry))
color_dict2 = {sec: cmap2(i) for i, sec in enumerate(unique_sectors_bdry)}

for sec in unique_sectors_bdry:
    subset = boundary_grids_info[boundary_grids_info["Sector"] == sec]
    subset.plot(ax=ax, facecolor=color_dict2[sec], edgecolor="black", linewidth=0.5, alpha=1.0, label=f"Sector {sec}")

berkeley_beats.boundary.plot(ax=ax, color="black", linewidth=1.5, linestyle="--", label="Sector Boundaries")

legend_patches2 = [
    Patch(facecolor=color_dict2[sec], edgecolor="black", label=f"Sector {sec}")
    for sec in unique_sectors_bdry
]
sector_boundary_patch = Patch(facecolor="none", edgecolor="black", label="Sector Boundaries", linestyle="--")
legend_patches2.append(sector_boundary_patch)

ax.legend(handles=legend_patches2, title="Boundary Grids by Sector", loc="upper left",
          bbox_to_anchor=(1.05, 1), borderaxespad=0.)
ax.set_title("Multi-Colored Boundary Grids with Sector Silhouette", fontsize=14)
ax.set_axis_off()
plt.show()

########################################
# 11) sector_neighbors (Dictionary)
########################################
sector_neighbors = {
    1: [2],
    2: [1, 3, 4, 14],
    3: [2, 4, 5, 11, 12, 14],
    4: [2, 3, 5, 6],
    5: [3, 4, 6, 9, 10, 11],
    6: [4, 5, 7, 8, 9],
    7: [6, 8],
    8: [6, 7, 9],
    9: [5, 6, 8, 10],
    10: [5, 9, 11],
    11: [3, 5, 10, 12],
    12: [3, 11, 13],
    13: [12, 14],
    14: [2, 3, 13]
}

print("\n**Done!** Everything should be consistent with int-type Sectors.\n")

print("=== Before merge ===")
print(sector_npps_sum)

berkeley_beats = berkeley_beats.merge(sector_npps_sum, on="Sector", how="left")

print("=== After merge ===")
print(sector_npps_sum)  # ë‹¤ì‹œ ì¶œë ¥

import geopandas as gpd
import pandas as pd

# -------------------------------
# 1. Boundary Pairs Info ìƒì„± í•¨ìˆ˜
# -------------------------------
def build_boundary_pairs_info(grid: gpd.GeoDataFrame) -> gpd.GeoDataFrame:
    """
    grid ë‚´ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ Sectorì— ì†í•œ Gridë¼ë¦¬ ì¸ì ‘(ë§ë‹¿ì•„ ìˆëŠ”)í•œ ê²½ìš°ë¥¼ ì°¾ì•„,
    í•´ë‹¹ Gridì™€ ë§ë‹¿ì€ ì´ì›ƒ Sector ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” DataFrameì„ ë°˜í™˜.
    ë°˜í™˜ ì»¬ëŸ¼: Grid_ID, Sector (í˜„ì¬), neighbor_sector, geometry
    """
    boundary_list = []
    for idx, row in grid.iterrows():
        from_sector = row["Sector"]
        # í•´ë‹¹ Gridì™€ ì¸ì ‘í•œ ëª¨ë“  Grid ì°¾ê¸°
        neighbors = grid[grid.geometry.touches(row.geometry)]
        # ì¸ì ‘í•œ Grid ì¤‘, Sectorê°€ ë‹¤ë¥¸ ê²½ìš°ë§Œ ì„ íƒ
        diff_sector_neighbors = neighbors[neighbors["Sector"] != from_sector]
        for n_idx, n_row in diff_sector_neighbors.iterrows():
            boundary_list.append({
                "Grid_ID": row["Grid_ID"],
                "Sector": from_sector,
                "neighbor_sector": n_row["Sector"],
                "geometry": row["geometry"]
            })
    # ë¦¬ìŠ¤íŠ¸ë¥¼ GeoDataFrameìœ¼ë¡œ ë³€í™˜
    boundary_pairs_info = gpd.GeoDataFrame(boundary_list, crs=grid.crs)
    # ì¤‘ë³µ ì œê±°
    boundary_pairs_info.drop_duplicates(subset=["Grid_ID", "Sector", "neighbor_sector"], inplace=True)
    return boundary_pairs_info

# -------------------------------
# 2. Sectorë³„ NPPS ì¬ê³„ì‚° í•¨ìˆ˜
# -------------------------------
def recalc_sector_npps_sum(grid: gpd.GeoDataFrame) -> pd.DataFrame:
    """
    gridë¥¼ ê¸°ì¤€ìœ¼ë¡œ Sectorë³„ NPPS í•©ê³„ë¥¼ ê³„ì‚°í•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜.
    ë°˜í™˜ ì»¬ëŸ¼: Sector, sector_total_npps
    """
    new_sum = grid.groupby("Sector")["total_npps"].sum().reset_index()
    new_sum.rename(columns={"total_npps": "sector_total_npps"}, inplace=True)
    return new_sum

# -------------------------------
# 3. NPPS ë³€í™” ì¶œë ¥ í•¨ìˆ˜
# -------------------------------
def print_npps_diff(old_sum: pd.DataFrame, new_sum: pd.DataFrame, sector_list: list):
    """
    old_sumê³¼ new_sumì„ mergeí•˜ì—¬, ì§€ì •í•œ sector_listì— í•´ë‹¹í•˜ëŠ” Sectorë“¤ì˜ NPPS ë³€í™”(ì°¨ì´)ë¥¼ ì¶œë ¥.
    """
    merged = old_sum.merge(new_sum, on="Sector", suffixes=("_old", "_new"))
    merged["npps_diff"] = merged["sector_total_npps_new"] - merged["sector_total_npps_old"]
    filtered = merged[merged["Sector"].isin(sector_list)]
    print(filtered[["Sector", "sector_total_npps_old", "sector_total_npps_new", "npps_diff"]])

# -------------------------------
# 4. Boundary ì´ë™ í•¨ìˆ˜ (Bulk ì´ë™)
# -------------------------------
def take_boundary_from_neighbor(grid: gpd.GeoDataFrame, boundary_pairs_info: gpd.GeoDataFrame,
                                  from_sector: int, to_sector: int) -> tuple[gpd.GeoDataFrame, list]:
    """
    from_sector â†’ to_sector ë¡œ ê²½ê³„ Gridë¥¼ ì „ë¶€ ì´ë™.
    ì¦‰, boundary_pairs_infoì—ì„œ (Sector=from_sector, neighbor_sector=to_sector)ì— í•´ë‹¹í•˜ëŠ”
    ëª¨ë“  Grid_IDë¥¼ ì°¾ì•„ì„œ, gridì—ì„œ í•´ë‹¹ Gridì˜ Sector ê°’ì„ to_sectorë¡œ ì¬í• ë‹¹.
    ë°˜í™˜: (ë³€ê²½ëœ grid, ì´ë™í•œ Grid_ID ëª©ë¡)
    """
    target_boundary = boundary_pairs_info[
        (boundary_pairs_info["Sector"] == from_sector) &
        (boundary_pairs_info["neighbor_sector"] == to_sector)
    ]
    moved_ids = target_boundary["Grid_ID"].unique()
    if len(moved_ids) == 0:
        return grid, []
    grid.loc[grid["Grid_ID"].isin(moved_ids), "Sector"] = to_sector
    return grid, moved_ids



# -------------------------------
# 6. ì „ì²´ ì‹¤í–‰ íë¦„
# -------------------------------
# ë¨¼ì €, boundary_pairs_info ìƒì„± (gridëŠ” ì•ì„œ ìƒì„±í•œ grid ê°ì²´, SectorëŠ” intë¡œ í†µì¼ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
boundary_pairs_info = build_boundary_pairs_info(grid)

import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt

# Assume required helper functions exist: build_boundary_pairs_info, recalc_sector_npps_sum, print_npps_diff

def give_bulk_boundaries_from_excess(grid: gpd.GeoDataFrame, sector_neighbors: dict, excess_sectors: list) -> tuple:
    """
    Excess NPPS sectors give all of their boundary grids to non-excess neighbors (bulk move),
    in the order of the fewest available neighbors first.
    Returns updated grid and list of all moved grid IDs.
    """
    boundary_pairs_info = build_boundary_pairs_info(grid)
    moved_all = []
    old_sum = recalc_sector_npps_sum(grid)

    eligible_donors = sorted(
        [s for s in excess_sectors],
        key=lambda s: len([n for n in sector_neighbors[s] if n not in excess_sectors])
    )

    for donor in eligible_donors:
        recipients = [n for n in sector_neighbors[donor] if n not in excess_sectors]
        print(f"\nğŸ” Sector {donor} attempts to give boundary grids to: {recipients}")

        for recipient in recipients:
            old_local_sum = recalc_sector_npps_sum(grid)
            grid, moved_ids = take_boundary_from_neighbor(grid, boundary_pairs_info, donor, recipient)
            # Check if moved_ids is not empty instead of directly evaluating its truthiness.
            if len(moved_ids) > 0: # Check if the list is not empty.
                moved_all.extend(moved_ids)
                new_local_sum = recalc_sector_npps_sum(grid)
                print(f"âœ… Moved {len(moved_ids)} grids from Sector {donor} â {recipient}")
                print_npps_diff(old_local_sum, new_local_sum, [donor, recipient])

    # Final summary
    final_sum = recalc_sector_npps_sum(grid)
    print("\nğŸ“Š Final NPPS (partial):")
    print(final_sum.sort_values("Sector"))

    return grid, moved_all

def plot_moved_grids(grid: gpd.GeoDataFrame, moved_ids: list, beats: gpd.GeoDataFrame, title="Grid Transfers from Excess Sectors"):
    fig, ax = plt.subplots(figsize=(12, 8))
    grid.plot(ax=ax, facecolor="lightgray", edgecolor="black", linewidth=0.2)
    grid[grid["Grid_ID"].isin(moved_ids)].plot(
        ax=ax, facecolor="yellow", edgecolor="red", linewidth=1, label="Moved Grids"
    )
    beats.boundary.plot(ax=ax, color="black", linestyle="--", linewidth=1.5)
    ax.set_title(title)
    ax.legend()
    ax.set_axis_off()
    plt.show()

# Step 1: Define excess sectors and neighbors
excess_sectors = [4, 6, 7]
# Make sure your 'grid' and 'sector_neighbors' are already defined

# Step 2: Run the bulk transfer function
grid, moved_ids = give_bulk_boundaries_from_excess(grid, sector_neighbors, excess_sectors)

# Step 3: Visualize result
plot_moved_grids(grid, moved_ids, berkeley_beats, title="Grid Transfers from Excess Sectors")

# -------------------------------
# Final: Colored Sector Map (After Excess Redistribution)
# -------------------------------
import matplotlib.pyplot as plt
from matplotlib.patches import Patch

fig, ax = plt.subplots(figsize=(12, 8))

# 1) Sector ëª©ë¡ ì¶”ì¶œ & ì»¬ëŸ¬ë§µ ì„¤ì •
unique_sectors = sorted(grid["Sector"].unique())
cmap = plt.cm.get_cmap("tab20", len(unique_sectors))
color_dict = {sec: cmap(i) for i, sec in enumerate(unique_sectors)}

# 2) Sectorë³„ë¡œ Grid ìƒ‰ìƒ êµ¬ë¶„
for sec in unique_sectors:
    subset = grid[grid["Sector"] == sec]
    subset.plot(
        ax=ax,
        facecolor=color_dict[sec],
        edgecolor="black",
        linewidth=0.5,
        alpha=1,
        label=f"Sector {sec}"
    )

# 3) Patrol Sector(Shapefile) ê²½ê³„ í‘œì‹œ (ì ì„ )
berkeley_beats.boundary.plot(
    ax=ax,
    color="black",
    linewidth=1.5,
    linestyle="--",
    label="Sector Boundaries"
)

# 4) ë²”ë¡€ (ê° Sector ìƒ‰ìƒ + ì ì„  ê²½ê³„)
legend_patches = [
    Patch(facecolor=color_dict[sec], edgecolor="black", label=f"Sector {sec}")
    for sec in unique_sectors
]
sector_boundary_patch = Patch(facecolor="none", edgecolor="black", label="Sector Boundaries", linestyle="--")
legend_patches.append(sector_boundary_patch)

ax.legend(
    handles=legend_patches,
    title="Final Reassigned Grid by Sector",
    loc="upper left",
    bbox_to_anchor=(1.05, 1),
    borderaxespad=0.
)

ax.set_title("Final Sector Assignments After Boundary Reassignment", fontsize=14)
ax.set_axis_off()
plt.show()

def take_bulk_boundaries_to_deficient(grid: gpd.GeoDataFrame, deficient_sector: int, neighbors: list) -> tuple:
    boundary_pairs_info = build_boundary_pairs_info(grid)
    moved_all = []
    old_sum = recalc_sector_npps_sum(grid)

    print(f"\nğŸŸ¢ Sector {deficient_sector} attempts to pull grids from neighbors: {neighbors}")

    for nbr in neighbors:
        old_local_sum = recalc_sector_npps_sum(grid)
        grid, moved_ids = take_boundary_from_neighbor(grid, boundary_pairs_info, nbr, deficient_sector)

        if len(moved_ids) > 0:
            moved_all.extend(moved_ids)
            new_local_sum = recalc_sector_npps_sum(grid)
            print(f"âœ… Moved {len(moved_ids)} grids from Sector {nbr} â {deficient_sector}")
            print_npps_diff(old_local_sum, new_local_sum, [nbr, deficient_sector])
        else:
            print(f"âš ï¸ No boundary grids moved from Sector {nbr} â {deficient_sector} (already transferred or not adjacent)")

    final_sum = recalc_sector_npps_sum(grid)
    print("\nğŸ“Š Final NPPS (partial):")
    print(final_sum.sort_values("Sector"))

    return grid, moved_all

# ì˜ˆì‹œ ì‹¤í–‰
deficient_sector = 12
neighbors_12 = [3, 11, 13]  # ë¯¸ë¦¬ ì •ì˜ëœ ì´ì›ƒë“¤

grid, moved_ids = take_bulk_boundaries_to_deficient(grid, deficient_sector, neighbors_12)

import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(12, 8))

# (A) ì „ì²´ Grid (ë°°ê²½)
grid.plot(
    ax=ax,
    facecolor="lightgray",
    edgecolor="black",
    linewidth=0.2,
    alpha=0.6
)

# (B) Sector 12ë¡œ ì´ë™ëœ Grid ê°•ì¡° (ë…¸ë€ìƒ‰ + ë¹¨ê°„ í…Œë‘ë¦¬)
moved_grids = grid[grid["Grid_ID"].isin(moved_ids)]
moved_grids.plot(
    ax=ax,
    facecolor="yellow",
    edgecolor="red",
    linewidth=1.0,
    alpha=1.0,
    label="Moved to Sector 12"
)

# (C) Patrol Sector ê³µì‹ ê²½ê³„ (ì ì„ )
berkeley_beats.boundary.plot(
    ax=ax,
    color="black",
    linewidth=1.2,
    linestyle="--",
    label="Sector Boundaries"
)

# ë²”ë¡€ ë° ì œëª©
ax.legend()
ax.set_title("Newly Assigned Grids to Sector 12 with Sector Boundaries", fontsize=14)
ax.set_axis_off()
plt.show()

from matplotlib.patches import Patch

fig, ax = plt.subplots(figsize=(12, 8))

# 1) ìœ ë‹ˆí¬ Sector ê°’ ë° ì»¬ëŸ¬ë§µ ì„¤ì •
unique_sectors = sorted(grid["Sector"].unique())
cmap = plt.cm.get_cmap("tab20", len(unique_sectors))
color_dict = {sec: cmap(i) for i, sec in enumerate(unique_sectors)}

# 2) ê° Sectorì— ìƒ‰ìƒ ì…í˜€ì„œ ì‹œê°í™”
for sec in unique_sectors:
    subset = grid[grid["Sector"] == sec]
    subset.plot(
        ax=ax,
        facecolor=color_dict[sec],
        edgecolor="black",
        linewidth=0.5,
        alpha=1.0,
        label=f"Sector {sec}"
    )

# 3) ê³µì‹ Sector ê²½ê³„ì„ 
berkeley_beats.boundary.plot(
    ax=ax,
    color="black",
    linewidth=1.2,
    linestyle="--",
    label="Sector Boundaries"
)

ax.legend(
    handles=legend_patches,
    title="Final Reassigned Grid by Sector",
    loc="upper left",
    bbox_to_anchor=(1.05, 1),
    borderaxespad=0.
)

ax.set_title("Final Sector Assignments After Boundary Reassignment", fontsize=14)
ax.set_axis_off()
plt.show()

grid.to_file("final_grid.shp")
print("Final optimized grid saved as final_grid.shp")

import numpy as np

def evaluate_npps_balance(old_sum: pd.DataFrame, new_sum: pd.DataFrame):
    # Merge old and new NPPS data
    merged = old_sum.merge(new_sum, on="Sector", suffixes=("_old", "_new"))

    # Calculate Variance
    old_variance = np.var(merged["sector_total_npps_old"])
    # Access the correct column for new variance calculation: sector_total_npps_new
    new_variance = np.var(merged["sector_total_npps_new"])

    # Max/Min Ratio
    old_max_min_ratio = merged["sector_total_npps_old"].max() / merged["sector_total_npps_old"].min()
    # Access the correct column for new max/min ratio calculation: sector_total_npps_new
    new_max_min_ratio = merged["sector_total_npps_new"].max() / merged["sector_total_npps_new"].min()

    # Deviation from Mean (%)
    old_mean = merged["sector_total_npps_old"].mean()
    # Access the correct column for new mean calculation: sector_total_npps_new
    new_mean = merged["sector_total_npps_new"].mean()

    merged["old_dev_from_mean(%)"] = ((merged["sector_total_npps_old"] - old_mean) / old_mean) * 100
    merged["new_dev_from_mean(%)"] = ((merged["sector_total_npps_new"] - new_mean) / new_mean) * 100

    print("\nğŸ“Š **NPPS Variance Comparison** ğŸ“Š")
    print(f"Before Optimization: {old_variance:.2f}")
    print(f"After Optimization:  {new_variance:.2f}")

    print("\nğŸ“Š **Max/Min Ratio Comparison** ğŸ“Š")
    print(f"Before Optimization: {old_max_min_ratio:.2f}")
    print(f"After Optimization:  {new_max_min_ratio:.2f}")

    print("\nğŸ“Š **Deviation from Mean (%) (First 5 Sectors)** ğŸ“Š")
    print(merged[["Sector", "old_dev_from_mean(%)", "new_dev_from_mean(%)"]].head(14))

    return merged  # ë°˜í™˜í•´ì„œ ì‹œê°í™” ë“± ì¶”ê°€ ë¶„ì„ ê°€ëŠ¥

# Assuming sector_npps_sum and grid are already defined
evaluation_result = evaluate_npps_balance(sector_npps_sum, recalc_sector_npps_sum(grid))

def normalized_score(value, worst, best):
    score_0_to_1 = (worst - value) / (worst - best)  # The lower the value, the better
    score_0_to_1 = max(min(score_0_to_1, 1), 0)  # Clamp the result between 0 and 1
    return round(score_0_to_1 * 9 + 1, 1)  # Convert to a 1~10 scale

# Assumption: Worst/Best thresholds for Variance (based on past data or domain knowledge)
worst_variance = 120000
best_variance = 60000

# Worst/Best thresholds for Max/Min Ratio
worst_ratio = 2.0
best_ratio = 1.25

# Current values
before_variance = 105089.55
after_variance = 65383.51
before_ratio = 1.76
after_ratio = 1.54

# Calculate Scores
before_var_score = normalized_score(before_variance, worst_variance, best_variance)
after_var_score = normalized_score(after_variance, worst_variance, best_variance)

before_ratio_score = normalized_score(before_ratio, worst_ratio, best_ratio)
after_ratio_score = normalized_score(after_ratio, worst_ratio, best_ratio)

# Final Score is the average of variance score and ratio score
before_final_score = round((before_var_score + before_ratio_score) / 2, 1)
after_final_score = round((after_var_score + after_ratio_score) / 2, 1)

print(f"ğŸ“Š **Workload Balance Score (Logic-Based)** ğŸ“Š")
print(f"Before Optimization: {before_final_score}/10")
print(f"After Optimization:  {after_final_score}/10")

########################################
# 6) Sectorë³„ NPPS í•©ê³„ & ì‹œê°í™” (Polygon ë‹¨ìœ„)
########################################
sector_npps_sum = grid.groupby("Sector")["total_npps"].sum().reset_index()
sector_npps_sum.rename(columns={"total_npps": "sector_total_npps"}, inplace=True)

# berkeley_beatsì™€ merge (ë‘˜ ë‹¤ intí˜• Sector)
berkeley_beats = berkeley_beats.merge(sector_npps_sum, on="Sector", how="left")

print("\nğŸ“Š **Sectorë³„ NPPS ì´í•©** ğŸ“Š\n")
print(berkeley_beats[["Sector", "sector_total_npps"]])

fig, ax = plt.subplots(figsize=(12, 8))
cmap = plt.cm.Reds
norm = mcolors.Normalize(vmin=berkeley_beats["sector_total_npps"].min(),
                         vmax=berkeley_beats["sector_total_npps"].max())

berkeley_beats.plot(column="sector_total_npps", cmap=cmap, norm=norm,
                    edgecolor="black", linewidth=1, alpha=0.7, ax=ax)

# Sector ì¤‘ì‹¬ì  ë¼ë²¨ í‘œì‹œ
berkeley_beats["centroid"] = berkeley_beats.geometry.centroid
for idx, row in berkeley_beats.iterrows():
    ax.text(row["centroid"].x, row["centroid"].y,
            f"{int(row['Sector'])}",
            fontsize=10, color="black", ha="center", va="center", fontweight="bold")

sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
sm.set_array([])
cbar = plt.colorbar(sm, ax=ax, fraction=0.03, pad=0.02)
cbar.set_label("Total WLS per Sector", fontsize=12)

ax.set_title("Sector-Level WLS Heatmap in Berkeley Police Patrol", fontsize=14)
ax.set_axis_off()
plt.show()

import geopandas as gpd
import matplotlib.pyplot as plt

# Load the final grid
grid = gpd.read_file("final_grid.shp")

# Create a figure
fig, ax = plt.subplots(figsize=(12, 8))

# Dissolve all grid cells by sector to get a single polygon per sector
sectors = grid.dissolve(by="Sector")

# Plot all sector boundaries in black
sectors.boundary.plot(
    ax=ax,
    color='black',
    linewidth=1.5,
    alpha=1.0
)

# Set title and turn off axis
ax.set_title("Berkeley Police Department Sectors", fontsize=14)
ax.set_axis_off()

# Show the plot
plt.tight_layout()
plt.show()

# Print basic information
print("\nSector Information:")
print(f"Number of sectors: {len(sectors)}")
print("\nSector IDs:")
print(sorted(sectors.index.tolist()))

import geopandas as gpd
import pandas as pd
import numpy as np
from shapely.ops import unary_union, nearest_points
from shapely.geometry import LineString, MultiLineString, Point, MultiPoint, Polygon, MultiPolygon
import os
import matplotlib.pyplot as plt
from shapely.geometry import shape, mapping
from shapely.ops import transform
from shapely.wkt import loads, dumps

def find_nearest_point_on_lines(point, lines, max_distance=50):
    """
    Find the nearest point on any line within max_distance meters.
    Returns None if no line is within max_distance.
    """
    min_dist = float('inf')
    nearest_point = None

    for line in lines:
        if line.distance(point) <= max_distance:
            p = line.interpolate(line.project(point))
            dist = point.distance(p)
            if dist < min_dist:
                min_dist = dist
                nearest_point = p

    return nearest_point if min_dist <= max_distance else None

def snap_polygon_to_streets(polygon, street_lines, tolerance=50):
    """
    Snap a single polygon's vertices to nearest street segments.
    """
    coords = list(polygon.exterior.coords)
    new_coords = []

    # Process each vertex
    for i, coord in enumerate(coords[:-1]):  # Skip last point (same as first)
        point = Point(coord)
        snapped = find_nearest_point_on_lines(point, street_lines, tolerance)

        if snapped:
            new_coords.append((snapped.x, snapped.y))
        else:
            new_coords.append(coord)

    # Close the polygon
    new_coords.append(new_coords[0])

    # Create new polygon
    return Polygon(new_coords)

def snap_to_streets(geom, streets, tolerance=50):
    """
    Snap polygon vertices to nearest street segments within tolerance distance.
    Handles both Polygon and MultiPolygon geometries.
    """
    # Extract all street geometries
    street_lines = [geom for geom in streets.geometry]

    if isinstance(geom, Polygon):
        return snap_polygon_to_streets(geom, street_lines, tolerance)
    elif isinstance(geom, MultiPolygon):
        # Handle each polygon in the MultiPolygon separately
        snapped_polys = []
        for poly in geom.geoms:
            snapped_poly = snap_polygon_to_streets(poly, street_lines, tolerance)
            snapped_polys.append(snapped_poly)
        return MultiPolygon(snapped_polys)
    else:
        raise ValueError(f"Unsupported geometry type: {type(geom)}")

def smooth_sectors(grid, streets, output_dir="output"):
    """
    Smooth sector boundaries by snapping to nearby streets and export as shapefile.

    Args:
        grid: GeoDataFrame containing the grid-based sectors
        streets: GeoDataFrame containing street centerlines
        output_dir: Directory to save output files
    """
    # Create output directory if it doesn't exist
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Ensure both datasets are in the same CRS
    streets = streets.to_crs(grid.crs)

    # Convert grid to polygons by sector
    sectors = []
    for sector in sorted(grid["Sector"].unique()):
        print(f"Processing Sector {sector}...")
        sector_grids = grid[grid["Sector"] == sector]
        # Dissolve all grid cells in the sector into a single polygon
        sector_polygon = unary_union(sector_grids.geometry)
        # Snap to streets
        snapped_polygon = snap_to_streets(sector_polygon, streets)
        sectors.append({
            "Sector": sector,
            "geometry": snapped_polygon
        })

    # Create GeoDataFrame of snapped sector polygons
    snapped_sectors = gpd.GeoDataFrame(sectors, crs=grid.crs)

    # Save the snapped sectors
    output_file = os.path.join(output_dir, "snapped_sectors.shp")
    snapped_sectors.to_file(output_file)

    print(f"Snapped sectors saved to {output_file}")
    return snapped_sectors

def visualize_comparison(original_grid, snapped_sectors, streets):
    """
    Create a visualization comparing original and snapped sectors with streets.
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

    # Original sectors with streets
    original_sectors = original_grid.dissolve(by="Sector")
    streets.plot(ax=ax1, color='gray', linewidth=0.5, alpha=0.5)
    original_sectors.boundary.plot(ax=ax1, color='black', linewidth=1.5)
    ax1.set_title("Original Sector Boundaries")
    ax1.set_axis_off()

    # Snapped sectors with streets
    streets.plot(ax=ax2, color='gray', linewidth=0.5, alpha=0.5)
    snapped_sectors.boundary.plot(ax=ax2, color='black', linewidth=1.5)
    ax2.set_title("Snapped Sector Boundaries")
    ax2.set_axis_off()

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    # Load the final grid and centerlines data
    print("Loading data...")
    grid = gpd.read_file("final_grid.shp")
    streets = gpd.read_file("Centerlines.shp")

    # Run the snapping process
    print("Snapping sectors to streets...")
    snapped = smooth_sectors(grid, streets)

    # Visualize the comparison
    print("Generating visualization...")
    visualize_comparison(grid, snapped, streets)

import geopandas as gpd
import matplotlib.pyplot as plt

# shapefile ë¶ˆëŸ¬ì˜¤ê¸°
shapefile_path = "output/snapped_sectors.shp"
gdf = gpd.read_file(shapefile_path)

# ë°ì´í„° í™•ì¸
print(gdf.head())

# ì‹œê°í™” - Sectorë³„ ìƒ‰ìƒ
gdf.plot(column='Sector', cmap='tab20', legend=True, edgecolor='black')

plt.show()

